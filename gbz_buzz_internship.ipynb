{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbDfsTeTjDZU",
        "outputId": "33ab9e13-aae7-4414-addc-702313378b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQNAVnAxjZ4c",
        "outputId": "f7dc54d0-2c96-4bcc-8b83-d4291e875eb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define data paths\n",
        "train_pos_dir = \"/content/drive/MyDrive/acllmdb/aclImdb/train/pos\"\n",
        "train_neg_dir = \"/content/drive/MyDrive/acllmdb/aclImdb/train/neg\"\n",
        "test_pos_dir = \"/content/drive/MyDrive/acllmdb/aclImdb/test/pos\"\n",
        "test_neg_dir = \"/content/drive/MyDrive/acllmdb/aclImdb/test/neg\"\n",
        "\n",
        "# Load data\n",
        "def load_data(directory):\n",
        "    data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "            data.append(file.read())\n",
        "    return data\n",
        "\n",
        "train_pos_data = load_data(train_pos_dir)\n",
        "train_neg_data = load_data(train_neg_dir)\n",
        "test_pos_data = load_data(test_pos_dir)\n",
        "test_neg_data = load_data(test_neg_dir)\n",
        "\n",
        "# Create labels\n",
        "train_labels = np.concatenate((np.ones(len(train_pos_data)), np.zeros(len(train_neg_data))))\n",
        "test_labels = np.concatenate((np.ones(len(test_pos_data)), np.zeros(len(test_neg_data))))\n",
        "\n",
        "# Combine positive and negative data\n",
        "train_data = train_pos_data + train_neg_data\n",
        "test_data = test_pos_data + test_neg_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Shi3ueHijHyg",
        "outputId": "3286bc66-fe6e-4e6d-83f9-c5521bf0ec62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b33adf82e79a>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtrain_neg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_neg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtest_pos_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pos_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtest_neg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_neg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Create labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b33adf82e79a>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "column_names=[\"reviews\"]"
      ],
      "metadata": {
        "id": "kak4A0lIl74p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos_df = pd.DataFrame(train_pos_data, columns = column_names)\n",
        "train_neg_df = pd.DataFrame(train_neg_data,columns = column_names)\n",
        "test_pos_df = pd.DataFrame(test_pos_data,columns = column_names)\n",
        "test_neg_df = pd.DataFrame(test_neg_data,columns = column_names)"
      ],
      "metadata": {
        "id": "De6VlutZmBYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos_df.insert(1,\"sentiment\",np.ones(len(train_pos_df), dtype=np.int8))\n",
        "train_neg_df.insert(1,\"sentiment\",np.zeros(len(train_pos_df), dtype=np.int8))\n",
        "test_pos_df.insert(1,\"sentiment\",np.ones(len(train_pos_df), dtype=np.int8))\n",
        "test_neg_df.insert(1,\"sentiment\",np.zeros(len(train_pos_df), dtype=np.int8))"
      ],
      "metadata": {
        "id": "y58bcHfCxgOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_collected = [train_pos_df, train_neg_df]\n",
        "train_data_df = pd.concat(train_data_collected, ignore_index=True)\n",
        "train_data_df.index += 1\n",
        "display(train_data_df)"
      ],
      "metadata": {
        "id": "01li5nVYmVLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_collected = [test_pos_df, test_neg_df]\n",
        "test_data_df = pd.concat(test_data_collected, ignore_index=True)\n",
        "test_data_df.index += 1\n",
        "display(test_data_df)"
      ],
      "metadata": {
        "id": "eGxNbLUhmbFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_df = pd.concat([train_data_df, test_data_df], ignore_index=True )\n",
        "concatenated_df.index += 1\n",
        "concatenated_df"
      ],
      "metadata": {
        "id": "hgpUBQsamej_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_shuffled = train_data_df.sample(frac=1,random_state = 1,).reset_index()\n",
        "test_data_shuffled = test_data_df.sample(frac=1,random_state = 1,).reset_index()"
      ],
      "metadata": {
        "id": "8STySba2lMwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_shuffled"
      ],
      "metadata": {
        "id": "S-49jepMlxRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_shuffled"
      ],
      "metadata": {
        "id": "6F3J5f5Wl05-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_small_df = train_data_shuffled[:1000]\n",
        "test_data_small_df = test_data_shuffled[:1000]"
      ],
      "metadata": {
        "id": "bEJjsGRPmp9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_small_df"
      ],
      "metadata": {
        "id": "i_TCYJTCqbGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_small_df"
      ],
      "metadata": {
        "id": "9udiGbCgr2Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_small_df = pd.concat([train_data_small_df, test_data_small_df], ignore_index=True )\n",
        "concatenated_small_df.index += 1\n",
        "concatenated_small_df"
      ],
      "metadata": {
        "id": "1utgYh4wpQOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reviews=train_data_small_df[\"reviews\"]\n",
        "train_sentiments=train_data_small_df[\"sentiment\"]\n",
        "test_reviews=test_data_small_df[\"reviews\"]\n",
        "test_sentiments=test_data_small_df[\"sentiment\"]\n",
        "print(train_reviews.shape,train_sentiments.shape)\n",
        "print(test_reviews.shape,test_sentiments.shape)"
      ],
      "metadata": {
        "id": "oOo--Q4gl1dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install beautifulsoup4\n",
        "!pip install spacy\n",
        "!pip install WordCloud\n",
        "!pip install textblob"
      ],
      "metadata": {
        "id": "-JL8nt5WnNsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "SL3QulYlnRiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "dN6f0Oh6nS4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "bsK3Q0WcnYBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "train_data_small_df['reviews']=train_data_small_df['reviews'].apply(denoise_text)\n",
        "test_data_small_df['reviews']=test_data_small_df['reviews'].apply(denoise_text)"
      ],
      "metadata": {
        "id": "AyPBhjUgnbMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "train_data_small_df['reviews']=train_data_small_df['reviews'].apply(remove_special_characters)\n",
        "test_data_small_df['reviews']=test_data_small_df['reviews'].apply(remove_special_characters)"
      ],
      "metadata": {
        "id": "Q-akjFFYntkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "#Apply function on review column\n",
        "train_data_small_df['reviews']=train_data_small_df['reviews'].apply(simple_stemmer)\n",
        "test_data_small_df['reviews']=test_data_small_df['reviews'].apply(simple_stemmer)"
      ],
      "metadata": {
        "id": "ae4GnAahn3IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set stopwords to english\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "train_data_small_df['reviews']=train_data_small_df['reviews'].apply(remove_stopwords)\n",
        "test_data_small_df['reviews']=test_data_small_df['reviews'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "ar5Iu4den_vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_train_reviews= train_data_small_df[\"reviews\"]\n",
        "norm_test_reviews=train_data_small_df[\"reviews\"]"
      ],
      "metadata": {
        "id": "AwmtU01noHVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_train_reviews[1]"
      ],
      "metadata": {
        "id": "80ROBdGyrOdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count vectorizer for bag of words\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "cv_test_reviews=cv.transform(norm_test_reviews)\n",
        "\n",
        "print('BOW_cv_train:',cv_train_reviews.shape)\n",
        "print('BOW_cv_test:',cv_test_reviews.shape)"
      ],
      "metadata": {
        "id": "j8aZ0j56oQ7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tfidf vectorizer\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "tv_test_reviews=tv.transform(norm_test_reviews)\n",
        "print('Tfidf_train:',tv_train_reviews.shape)\n",
        "print('Tfidf_test:',tv_test_reviews.shape)"
      ],
      "metadata": {
        "id": "wO88K1lsoWKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#labeling the sentient data\n",
        "lb=LabelBinarizer()\n",
        "#transformed sentiment data\n",
        "sentiment_data=lb.fit_transform(concatenated_small_df['sentiment'])\n",
        "print(sentiment_data.shape)"
      ],
      "metadata": {
        "id": "_ntUwU0ioaz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentiments=sentiment_data[:1000]\n",
        "test_sentiments=sentiment_data[:1000]"
      ],
      "metadata": {
        "id": "Z3A4FofkpbJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_sentiments)"
      ],
      "metadata": {
        "id": "x_BwdH8qpoff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "#Fitting the model for Bag of words\n",
        "lr_bow=lr.fit(cv_train_reviews,train_sentiments.ravel())\n",
        "print(lr_bow)\n",
        "#Fitting the model for tfidf features\n",
        "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments.ravel())\n",
        "print(lr_tfidf)"
      ],
      "metadata": {
        "id": "Y_biKezSpsOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting the model for bag of words\n",
        "lr_bow_predict=lr.predict(cv_test_reviews)\n",
        "print(lr_bow_predict)\n",
        "##Predicting the model for tfidf features\n",
        "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
        "print(lr_tfidf_predict)"
      ],
      "metadata": {
        "id": "0p63A8PRqCfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy score for bag of words\n",
        "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
        "print(\"lr_bow_score :\",lr_bow_score)\n",
        "#Accuracy score for tfidf features\n",
        "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
        "print(\"lr_tfidf_score :\",lr_tfidf_score)"
      ],
      "metadata": {
        "id": "zkE1kJQhqDFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification report for bag of words\n",
        "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
        "print(lr_bow_report)\n",
        "\n",
        "#Classification report for tfidf features\n",
        "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(lr_tfidf_report)"
      ],
      "metadata": {
        "id": "gnwM501qqIIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix for bag of words\n",
        "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "#confusion matrix for tfidf features\n",
        "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)"
      ],
      "metadata": {
        "id": "VRQgLDQOqK6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "AM1m2eo75WtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Step 1: Load and organize the dataset\n",
        "train_pos_dir = \"/content/drive/MyDrive/acllmdb/aclImdb/train/\"\n",
        "train_neg_dir = \"/content/drive/MyDrive/acllmdb/aclImdb/train/\"\n",
        "test_pos_dir = \"/content/drive/MyDrive/acllmdb/aclImdb/test/\"\n",
        "test_neg_dir = \"/content/drive/MyDrive/acllmdb/aclImdb/test/\"\n",
        "positive_train_files = [os.path.join(train_pos_dir, 'pos', f) for f in os.listdir(os.path.join(train_pos_dir, 'pos'))]\n",
        "negative_train_files = [os.path.join(train_neg_dir, 'neg', f) for f in os.listdir(os.path.join(train_neg_dir, 'neg'))]\n",
        "positive_test_files = [os.path.join(test_pos_dir, 'pos', f) for f in os.listdir(os.path.join(test_pos_dir, 'pos'))]\n",
        "negative_test_files = [os.path.join(test_neg_dir, 'neg', f) for f in os.listdir(os.path.join(test_neg_dir, 'neg'))]\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub('\\[[^]]*\\]', '', text)\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load and preprocess training data\n",
        "X_train = []\n",
        "y_train = []\n",
        "for file_path in positive_train_files:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        review = f.read()\n",
        "        X_train.append(preprocess_text(review))\n",
        "        y_train.append(1)  # Positive sentiment label\n",
        "for file_path in negative_train_files:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        review = f.read()\n",
        "        X_train.append(preprocess_text(review))\n",
        "        y_train.append(0)  # Negative sentiment label\n",
        "\n",
        "# Load and preprocess test data\n",
        "X_test = []\n",
        "y_test = []\n",
        "for file_path in positive_test_files:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        review = f.read()\n",
        "        X_test.append(preprocess_text(review))\n",
        "        y_test.append(1)  # Positive sentiment label\n",
        "for file_path in negative_test_files:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        review = f.read()\n",
        "        X_test.append(preprocess_text(review))\n",
        "        y_test.append(0)  # Negative sentiment label\n",
        "\n",
        "# Step 3: Feature Extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 4: Training\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 5: Evaluation\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISwCoBPRz6lf",
        "outputId": "f12af9d3-89a0-4774-cb16-a3f5f3103fd0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-9-6e209278ab2a>:18: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88\n",
            "Precision: 0.88\n",
            "Recall: 0.88\n",
            "F1-score: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of grid searching key hyperparametres for logistic regression\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# define dataset\n",
        "# define models and parameters\n",
        "model = LogisticRegression()\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "penalty = ['l2']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "grid_result = grid_search.fit(X_train_tfidf, y_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "BrrAWVF20eLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC()\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 5: Evaluation\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "yS2CA6n89hI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of grid searching key hyperparametres for SVC\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "# define model and parameters\n",
        "model = SVC()\n",
        "kernel = ['poly', 'rbf', 'sigmoid']\n",
        "C = [50, 10, 1.0, 0.1, 0.01]\n",
        "gamma = ['scale']\n",
        "# define grid search\n",
        "grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "grid_result = grid_search.fit(X_train_tfidf, y_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "KrWVlm079KOw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}